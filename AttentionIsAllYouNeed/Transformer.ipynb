{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def positional_encoding(shape,debug=False):\n",
    "    #shape =input.shape# tf.TensorShape(data.shape) #tf.shape(data) \n",
    "    dim_sequence = tf.cast(shape[-2], tf.float32)\n",
    "    dim_embedding = tf.cast(shape[-1]/2, tf.float32)\n",
    "    normfactor = dim_sequence/15\n",
    "    dim_mesh, seq_mesh = tf.meshgrid(tf.range(0, dim_embedding), tf.range(0, dim_sequence))\n",
    "    \n",
    "    phase = tf.pow(tf.cast(seq_mesh, tf.float32)/normfactor, 2*tf.cast(dim_mesh,tf.float32)/dim_embedding)\n",
    "    \n",
    "    stacked = tf.concat([tf.sin(phase), tf.cos(phase)], axis = 1)\n",
    "    if debug:\n",
    "        print(\"stacked.shape\", stacked.shape)\n",
    "        #print(\"shape\", shape)\n",
    "        print(\"transposed image\")\n",
    "        plt.imshow(tf.transpose(stacked), interpolation='nearest')\n",
    "        plt.show()\n",
    "    \n",
    "    stacked = tf.reshape(stacked, (1,shape[1],shape[2])) # tf.Dimension(None)\n",
    "#    return tf.convert_to_tensor(stacked, dtype=np.float32)\n",
    "#     print(shape[0])\n",
    "    return tf.tile(stacked, (shape[0],1,1))\n",
    "\n",
    "tf.constant(np.ones([32,40,58]))\n",
    "inp=tf.keras.Input((40,58), batch_size=32)\n",
    "\n",
    "v=positional_encoding(tf.TensorShape(inp.shape), True)\n",
    "\n",
    "print(v,v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_attention(input, debug=False):\n",
    "    if debug:\n",
    "        print(\"preprocess_attention\")\n",
    "    # q,k,v (steps, attention_dim * query_count)\n",
    "    query_count = 10\n",
    "    attention_dim=input.shape[-1]\n",
    "    head_dim=attention_dim\n",
    "    \n",
    "    # create queries, keys and values\n",
    "    # attention_input = tf.keras.layers.Dense(units=(1+1+2)*attention_dim*query_count, activation=tf.nn.relu)(input)\n",
    "\n",
    "    k = tf.keras.layers.Dense(attention_dim*query_count)(input)\n",
    "    q = tf.keras.layers.Dense(attention_dim*query_count)(input)\n",
    "    v = tf.keras.layers.Dense(2*attention_dim*query_count)(input)\n",
    "    \n",
    "#     q = attention_input[:,:,attention_dim*query_count:2*attention_dim*query_count]\n",
    "#     v = attention_input[:,:,2*attention_dim*query_count:]\n",
    "\n",
    "    if debug:\n",
    "        print(\"k,q,v\",k.shape.as_list(),q.shape.as_list(),v.shape.as_list())\n",
    "    \n",
    "    return k,q,v\n",
    "\n",
    "def multihead_attention(k,q,v,attention_dim,heads=20,debug=False):\n",
    "    if debug:\n",
    "        print(\"multihead_attention\")\n",
    "        print(\"kqv shapes, heads\", k.shape, q.shape, v.shape, heads)\n",
    "    head_dim=attention_dim\n",
    "    value_dim=2*head_dim\n",
    "    seqlength=k.shape[1]\n",
    "    dk = tf.cast(attention_dim,tf.float32)\n",
    "\n",
    "    # project original q,k,v onto #heads\n",
    "    K = tf.keras.layers.Dense(head_dim*heads, activation=None)(k)\n",
    "    Q = tf.keras.layers.Dense(head_dim*heads, activation=None)(q)\n",
    "    V = tf.keras.layers.Dense(value_dim*heads, activation=None)(v)\n",
    "\n",
    "    K = tf.keras.layers.Reshape((seqlength, heads, head_dim))(K)\n",
    "    Q = tf.keras.layers.Reshape((seqlength, heads, head_dim))(Q)\n",
    "    V = tf.keras.layers.Reshape((seqlength, heads, value_dim))(V)\n",
    "\n",
    "#     # Q,K,V (steps, head_dim, heads)\n",
    "#     print(Q.shape, K.shape, V.shape)\n",
    "#     plt.imshow(K[0,:,:])\n",
    "#     plt.show()\n",
    "#     plt.imshow(Q[0,:,:])\n",
    "#     plt.show()\n",
    "#     plt.imshow(V[0,:,:])\n",
    "#     plt.show()\n",
    "    \n",
    "    Kt=tf.keras.layers.Permute((1,3,2))(K)\n",
    "    print(Q.shape.as_list(), Kt.shape.as_list())\n",
    "    attention_logits = tf.tensordot(Q, Kt, [3,2]) / tf.sqrt(dk)\n",
    "    if debug:\n",
    "        print(\"Query shape, Keys' shape, attention_logits shape\")\n",
    "        print(Q.shape.as_list(), Kt.shape.as_list(), attention_logits.shape.as_list())\n",
    "    # attention (steps, heads, heads)\n",
    "    attention = tf.nn.softmax(attention_logits, dim=2)\n",
    "    #if debug:\n",
    "        # investigate here\n",
    "        #print(\"sum attention softmax\")\n",
    "        #print(tf.reduce_sum(tf.reduce_sum(attention, (2,)), (0,)))\n",
    "    # selected_values (steps, heads, value_dim)\n",
    "    # each head selects one value as a linear combination of the original values\n",
    "    print(attention.shape, V.shape)\n",
    "    selected_values = tf.keras.backend.dot(attention, V)\n",
    "    # concat (steps, heads*value_dim)\n",
    "    concat = tf.keras.layers.Reshape((seqlength, heads*value_dim))(selected_values)\n",
    "\n",
    "    # project back into the dimension of the attention_vector\n",
    "    output = tf.keras.layers.Dense(units=attention_dim, activation=None)(concat)\n",
    "    if debug:\n",
    "        print(\"attention:\", attention.shape, \", selected_values:\", selected_values.shape, \", concat:\", concat.shape, \", output:\", output.shape)\n",
    "\n",
    "    return output\n",
    "\n",
    "attention_dim=4\n",
    "t=tf.convert_to_tensor(np.ones([32,10,attention_dim]), dtype=tf.float32)\n",
    "k,q,v=preprocess_attention(t, True)\n",
    "mh= multihead_attention(k,q,v,attention_dim,debug=True)\n",
    "\n",
    "# the following will be mostly linear array of 100 (sum softmax outputs then along step axis)\n",
    "print(mh.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tf.keras.backend.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_module(input, output=None, debug=False):\n",
    "\n",
    "    attention_dim=input.shape[-1]\n",
    "    #input=tf.convert_to_tensor(input, dtype=tf.float32)\n",
    "    if debug:\n",
    "        print(\"module input\",input.shape.as_list())\n",
    "\n",
    "    res=input\n",
    "\n",
    "    # we create query_count queries, keys and values\n",
    "    if output is not None:\n",
    "        k,q,v = preprocess_attention(output, debug=debug)\n",
    "        output = output + multihead_attention(k,q,v,attention_dim, debug=debug)\n",
    "\n",
    "    k,q,v = preprocess_attention(input, debug=debug)\n",
    "\n",
    "    if output is not None:\n",
    "        q = output\n",
    "        res = output\n",
    "        \n",
    "    res = res + multihead_attention(k,q,v,attention_dim, debug=debug)\n",
    "    if debug:\n",
    "        print(\"attention_output\",res.shape.as_list())\n",
    "\n",
    "    res = tf.keras.layers.Dropout(.1)(res)\n",
    "\n",
    "    norm = tf.contrib.layers.layer_norm(res)\n",
    "    \n",
    "    net = norm + tf.keras.layers.Dense(units=norm.shape[-1], activation=None)(norm)\n",
    "    norm = tf.contrib.layers.layer_norm(net)\n",
    "\n",
    "    return norm\n",
    "\n",
    "# i=np.ones([32,1000,4], np.float32)\n",
    "# M=transformer_module(i)\n",
    "# print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(input, layers, debug=False):\n",
    "    for i in range(layers):\n",
    "        if debug:\n",
    "            print(\"input\",input.shape)\n",
    "        input = transformer_module(input, debug=debug)\n",
    "    return input\n",
    "\n",
    "def decode(z, output, layers):\n",
    "    for i in range(layers):\n",
    "        output = transformer_module(z, output, debug=debug)\n",
    "    return output\n",
    "\n",
    "def encoder_decoder(input, output, layers):\n",
    "    z = encode(input,layers)\n",
    "    return decode(z,output,layers)\n",
    "    \n",
    "def transformer_net(input, output=None, layers=1, \n",
    "                    activate=tf.keras.layers.Activation(activation='softmax'),\n",
    "                    internal_dim=10, debug=False):\n",
    "    #model = tf.keras.model.Sequential()\n",
    "    \n",
    "#     input = tf.convert_to_tensor(input, dtype=tf.float32)\n",
    "#     if output is not None:\n",
    "#         output = tf.convert_to_tensor(output, dtype=tf.float32)\n",
    "\n",
    "    inpL = tf.keras.layers.Dense(units=internal_dim, activation=None)\n",
    "    inp = inpL(input)\n",
    "    \n",
    "    # positional encoding\n",
    "    lpe = tf.keras.layers.Lambda(lambda x: positional_encoding(x.shape), output_shape=inp.shape.as_list()[1:])\n",
    "    print(inp.shape, lpe(inp).shape)\n",
    "    inp = tf.keras.layers.add([inp,lpe(inp)])\n",
    "    \n",
    "    retoutput = encode(inp, layers, debug)\n",
    "\n",
    "    if output is not None:\n",
    "        output = tf.keras.layers.Masking(mask_value=0.)(output)\n",
    "        output = tf.keras.layers.Dense(units=internal_dim, activation=None)(output)\n",
    "        output = tf.keras.layers.add([output,lpe(output)])\n",
    "        retoutput = decode(retoutput,output, layers)\n",
    "    #print(retoutput.shape) #10,40,100\n",
    "    retoutput = tf.keras.layers.Dense(input.shape[-1])(retoutput)\n",
    "    #print(retoutput.shape) #10,40,57\n",
    "    if activate is not None:\n",
    "        retoutput = activate(retoutput)\n",
    "\n",
    "    if debug:\n",
    "        print(retoutput.shape)\n",
    "\n",
    "    #retoutput = tf.keras.models.Model(tf.keras.Input(input), retoutput)    \n",
    "    return retoutput\n",
    "    \n",
    "input=np.reshape(np.arange(0,512),(512,1),np.float32)\n",
    "\n",
    "# output=np.zeros(input.shape,np.float32)\n",
    "# output[:-1] = input[1:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import get_file\n",
    "import io\n",
    "\n",
    "def prepare_dataset():\n",
    "    path = get_file(\n",
    "        'nietzsche.txt',\n",
    "        origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "    with io.open(path, encoding='utf-8') as f:\n",
    "        text = f.read().lower()\n",
    "    print('corpus length:', len(text))\n",
    "\n",
    "    chars = sorted(list(set(text)))\n",
    "    print('total chars:', len(chars))\n",
    "    char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "    indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "    chars = sorted(list(set(text)))\n",
    "    print('total chars:', len(chars))\n",
    "    char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "    indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "    # cut the text in semi-redundant sequences of maxlen characters\n",
    "    maxlen = 40\n",
    "    step = 3\n",
    "    sentences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(text) - maxlen, step):\n",
    "        sentences.append(text[i: i + maxlen])\n",
    "        next_chars.append(text[i + maxlen])\n",
    "    print('nb sequences:', len(sentences))\n",
    "\n",
    "    print('Vectorization...')\n",
    "    x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.float32)\n",
    "    y = np.zeros((len(sentences), len(chars)), dtype=np.float32)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "        y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "    return x,y\n",
    "\n",
    "X_train,y_train=prepare_dataset()\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0], y_train[0])\n",
    "print(X_train[0,:,:].shape,y_train[0,:].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def loss(logits,labels):\n",
    "    # logits.shape:64,40,57  and logits[:,-1].shape:64,57\n",
    "    print(logits.shape,logits[:,-1].shape)\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits[:,-1],labels=labels)\n",
    "    red = tf.reduce_sum(loss)\n",
    "    return red\n",
    "\n",
    "#l = loss(X_train[0], y_train[0])\n",
    "batch_size=64\n",
    "epochs=90\n",
    "print(X_train.shape)\n",
    "X=tf.keras.Input(X_train[0].shape, batch_size=batch_size)\n",
    "print(\"input shape\", X.shape)\n",
    "logits = transformer_net(X, activate=None, debug=True)\n",
    "model = tf.keras.Model(inputs=(X,),outputs=logits)\n",
    "\n",
    "l = loss(model,y)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"epoch \", epoch)\n",
    "    for i in range(0,len(X_train),batch_size):\n",
    "        with tf.GradientTape() as tape:\n",
    "            l(X_train[i:i+batch_size,:,:], y_train[i:i+batch_size,:])\n",
    "\n",
    "        print(\"loss: %d\", l)\n",
    "\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-4)\n",
    "        variables=tape.watched_variables()\n",
    "        grads = tape.gradient(l, variables)\n",
    "        \n",
    "        print(variables[1].name, variables[1], grads[1])\n",
    "        optimizer.apply_gradients(zip(grads, variables),\n",
    "                                    global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for v in variables:\n",
    "    print(v)\n",
    "\n",
    "\n",
    "print(container.variables())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def input_fn(images, labels, epochs, batch_size):\n",
    "    # Convert the inputs to a Dataset. (E)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    # Shuffle, repeat, and batch the examples. (T)\n",
    "    SHUFFLE_SIZE = 5000\n",
    "    ds = ds.shuffle(SHUFFLE_SIZE).repeat(epochs).batch(batch_size)\n",
    "    ds = ds.prefetch(2)\n",
    "    # Return the dataset. (L)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def my_model(features, labels, mode, params):\n",
    "#     \"\"\"DNN with three hidden layers, and dropout of 0.1 probability.\"\"\"\n",
    "#     # Create three fully connected layers each layer having a dropout\n",
    "#     # probability of 0.1.\n",
    "#     #net = tf.feature_column.input_layer(features, params['input_1'])\n",
    "    \n",
    "#     # Compute predictions.\n",
    "#     model=MyModel(features, params)\n",
    "#     model.compile()\n",
    "    \n",
    "#     predicted_classes = tf.argmax(model.output,axis= 1)\n",
    "    \n",
    "#     #model = tf.keras.Model(inputs=(features['input_1'],),outputs=(logits, predicted_classes) )\n",
    "    \n",
    "#     if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        \n",
    "#         predictions = {\n",
    "#             'class_ids': predicted_classes[:, tf.newaxis],\n",
    "#             'probabilities': tf.nn.softmax(logits),\n",
    "#             'logits': logits,\n",
    "#         }\n",
    "#         return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "#     print(logits.shape)\n",
    "#     # Compute loss.\n",
    "#     loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "#     # Compute evaluation metrics.\n",
    "#     accuracy = tf.metrics.accuracy(labels=labels,\n",
    "#                                    predictions=predicted_classes,\n",
    "#                                    name='acc_op')\n",
    "#     metrics = {'accuracy': accuracy}\n",
    "#     tf.summary.scalar('accuracy', accuracy[1])\n",
    "\n",
    "#     if mode == tf.estimator.ModeKeys.EVAL:\n",
    "#         return tf.estimator.EstimatorSpec(\n",
    "#             mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "#     # Create training op.\n",
    "#     assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "#     optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "#     train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "#     return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\n",
    "\n",
    "def train_input_fn(X, y, batch_size):\n",
    "    inputs=({'inputlayer': tf.reshape(X, [X.shape[0],-1])},y)\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "    batch= ds.shuffle(100).repeat().batch(batch_size)\n",
    "    return batch\n",
    "    \n",
    "def eval_input_fn(X, y, batch_size):\n",
    "    inputX={'inputlayer': tf.reshape(X, [X.shape[0],-1])}\n",
    "    if y is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = inputX\n",
    "    else:\n",
    "        inputs = (inputX, y)\n",
    "        \n",
    "    ds = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = ds.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "def MultiHeadAttention(Q,K,V):\n",
    "    logits = Q * tf.transpose(K) / tf.sqrt(tf.shape(K)[1])\n",
    "    sm = tf.nn.softmax(logits)\n",
    "    tf.concat(sm) * V\n",
    "\n",
    "    \n",
    "def comp_model():\n",
    "    logits, probs = kerasModel()\n",
    "    model = tf.keras.Model(inputs, [ logits, probs])\n",
    "    #loss=tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=model)\n",
    "\n",
    "    opt = tf.train.AdamOptimizer()\n",
    "    model.compile(optimizer=opt, loss='mse',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = kerasModel()\n",
    "    \n",
    "estimator = tf.keras.estimator.model_to_estimator(model)\n",
    "\n",
    "batch_size=32\n",
    "train_steps=200#60000*4\n",
    "\n",
    "# Fetch the data\n",
    "(train_x, train_y),(test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "train_x=train_x.astype(np.float)\n",
    "train_y = train_y.astype(np.int32)\n",
    "test_x = test_x.astype(np.float)\n",
    "test_y = test_y.astype(np.int32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the Model.\n",
    "estimator.train(\n",
    "    input_fn=lambda: train_input_fn(train_x, train_y, batch_size),\n",
    "    steps=train_steps)\n",
    "\n",
    "# Evaluate the model.\n",
    "eval_result = estimator.evaluate(\n",
    "    input_fn=lambda: eval_input_fn(test_x, test_y, batch_size))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n",
    "\n",
    "# Generate predictions from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_x=train_x[0:32].reshape((32,-1))\n",
    "print(predict_x.shape)\n",
    "predictions = estimator.predict(\n",
    "    input_fn=lambda:eval_input_fn(predict_x,None,\n",
    "                                            batch_size=batch_size))\n",
    "for pred_dict in predictions[1]:\n",
    "    print(pred_dict)\n",
    "\n",
    "\n",
    "for pred_dict, expec in zip(predictions, expected):\n",
    "    template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')\n",
    "\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "    print(template.format(iris_data.SPECIES[class_id],\n",
    "                          100 * probability, expec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
